{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Classical Quantum AutoEncoder for anomaly detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qiskit Implementation\n",
    "Run this section if you have qiskit-machine-learning installed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQC\n",
    "The VQC implemented is the number 10 of the reference paper, which essentially is a stack Pauli $YX$ rotation gate and a circular series of controlled $CX$, stacked while alternated with an encoding based on the Pauli $X$ rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit import QuantumCircuit, Gate, ParameterVector\n",
    "from qiskit.opflow.expectations import PauliExpectation\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "def get_encoding_block(nqubits: int, features: ParameterVector) -> Gate:\n",
    "    \"\"\" n parameters required \"\"\"\n",
    "    assert len(features) == nqubits \n",
    "    block = QuantumCircuit(nqubits, name=\"Encoding Block\")\n",
    "    for i in range(nqubits):\n",
    "        block.rx(features[i], i)\n",
    "    return block.to_gate()\n",
    "\n",
    "def get_ansatz_block(nqubits: int, parameters: ParameterVector) -> Gate:\n",
    "    \"\"\"\n",
    "    need 2n parameters\n",
    "    \"\"\"\n",
    "    assert nqubits * 2 == len(parameters)\n",
    "    block = QuantumCircuit(nqubits, name=\"Ansatz Block\")\n",
    "    for i in range(nqubits):\n",
    "        block.ry(parameters[i], i)\n",
    "        block.rx(parameters[i + nqubits], i)\n",
    "    if nqubits > 1:\n",
    "        block.cx(nqubits - 1, 0)\n",
    "        for i in range(nqubits - 1):\n",
    "            block.cx(i, i + 1)\n",
    "    return block.to_gate()\n",
    "\n",
    "def get_ansatz(nqubits: int, parameters: ParameterVector, features: ParameterVector, reps: int=3) -> QuantumCircuit:\n",
    "    assert len(parameters) == reps * 2 * nqubits\n",
    "    ansatz = QuantumCircuit(nqubits)\n",
    "    ansatz.compose(get_ansatz_block(nqubits, parameters[:2 * nqubits]), range(nqubits), inplace=True)\n",
    "    for i in range(1, reps):\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_encoding_block(nqubits, features), range(nqubits), inplace=True)\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_ansatz_block(nqubits, parameters[2 * nqubits * i :2 * nqubits * (i + 1)]), range(nqubits), inplace=True)\n",
    "    return ansatz\n",
    "\n",
    "def get_ansatz_ws(nqubits: int, parameters: ParameterVector, features: ParameterVector, reps: int=3) -> QuantumCircuit:\n",
    "    assert len(parameters) == 2 * nqubits\n",
    "    ansatz = QuantumCircuit(nqubits)\n",
    "    ansatz.compose(get_ansatz_block(nqubits, parameters), range(nqubits), inplace=True)\n",
    "    for i in range(1, reps):\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_encoding_block(nqubits, features), range(nqubits), inplace=True)\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_ansatz_block(nqubits, parameters), range(nqubits), inplace=True)\n",
    "    return ansatz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.utils import algorithm_globals\n",
    "algorithm_globals.random_seed = 528491\n",
    "\n",
    "size = (10, 4)\n",
    "data = algorithm_globals.random.random(size)\n",
    "nqubits = data.shape[1]\n",
    "print(data, nqubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ParameterVector(\"x\", data.shape[1])\n",
    "weights = ParameterVector(\"theta\", nqubits * 2)\n",
    "circuit_ws = get_ansatz_ws(nqubits, weights, input)\n",
    "circuit_ws.decompose().draw(\"mpl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weights_ws = algorithm_globals.random.random(2 * nqubits)\n",
    "print(random_weights_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "\n",
    "sqnn_ws = SamplerQNN(\n",
    "    circuit=circuit_ws,\n",
    "    input_params=input,\n",
    "    weight_params=weights,\n",
    "    interpret=lambda x: \"{:b}\".format(x).count('1') % 2 == 0, # parity check\n",
    "    output_shape=2\n",
    ")\n",
    "print(sqnn_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_qnn_forward = sqnn_ws.forward(data[0], random_weights_ws) # require encoding + ansatz parameters, result is a ndarray\n",
    "print(f\"Forward pass result for SamplerQNN: {sampler_qnn_forward}. \\nShape: {sampler_qnn_forward.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = ParameterVector(\"x1\", data.shape[1])\n",
    "weights1 = ParameterVector(\"theta1\", nqubits * 2 * 3)\n",
    "circuit = get_ansatz(nqubits, weights1, input1)\n",
    "circuit.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity(x):\n",
    "    print(x, str(x), \"{:b}\".format(x), \"{:b}\".format(x).count('1') % 2)\n",
    "    return \"{:b}\".format(x).count('1') % 2\n",
    "\n",
    "def custom_interpret(x):\n",
    "    return \"{:b}\".format(x).count('1') % 4\n",
    "\n",
    "\n",
    "sqnn = SamplerQNN(\n",
    "    circuit=circuit,\n",
    "    input_params=input1,\n",
    "    weight_params=weights1,\n",
    "    interpret=lambda x: custom_interpret(x), # parity check\n",
    "    output_shape=4\n",
    ")\n",
    "print(sqnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weights = algorithm_globals.random.random(len(weights1))\n",
    "sampler_qnn_forward = sqnn.forward(data[0], random_weights) # require encoding + ansatz parameters, result is a ndarray\n",
    "print(f\"Forward pass result for SamplerQNN: {sampler_qnn_forward}. \\nShape: {sampler_qnn_forward.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.opflow.expectations import PauliExpectation\n",
    "\n",
    "def get_Z_expectation_qubitwise(nqubits: int) -> list:\n",
    "    \"\"\"\n",
    "        :nqubits\n",
    "        the number of qubits to evaluate\n",
    "        :return\n",
    "        the qubitwise observables for each Z expectation value, which follows the formula P(1) = (1 - Exp) / 2\n",
    "    \"\"\"\n",
    "    obs = []\n",
    "    for i in range(nqubits):\n",
    "        string = \"I\" * i + \"Z\" + \"I\" * (nqubits - (i + 1))\n",
    "        obs.append(SparsePauliOp.from_list([(string, 1)]))\n",
    "    return obs\n",
    "\n",
    "ob = get_Z_expectation_qubitwise(4)\n",
    "print(ob)\n",
    "\n",
    "# observable1 = SparsePauliOp.from_list([(\"Z\", 1), (\"Z\", 1), (\"Z\", 1), (\"I\", 1)])\n",
    "eqnn_ws = EstimatorQNN(\n",
    "    circuit=circuit_ws,\n",
    "    input_params=input,\n",
    "    weight_params=weights,\n",
    "    observables=ob\n",
    ")\n",
    "\n",
    "print(eqnn_ws)\n",
    "circuit_ws.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eqnn_ws.forward(data[0], random_weights_ws)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic AutoEncoder Wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Qiskit & TorchConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class HAE(nn.Module):\n",
    "    \"\"\"\n",
    "        general structure is:\n",
    "        - encoder, FC input_size -> 54 -> 4\n",
    "        - qnn\n",
    "        - decoder, FC 4 -> 54 -> input_size\n",
    "        - tanh activations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qnn, input_size: int, nqubits: int = 4) -> None:\n",
    "        super(HAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, nqubits),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.vqc = TorchConnector(qnn)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(nqubits, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, input_size)\n",
    "        )\n",
    "\n",
    "        self.isolation_forest = IsolationForest()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.vqc(X)\n",
    "        X = (torch.ones_like(X) - X) / 2\n",
    "        X = self.decoder(X)\n",
    "        return X\n",
    "    \n",
    "    def encode(self, X):\n",
    "        C = self.encoder(X)\n",
    "        return self.vqc(C)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, X, fit: bool = False):\n",
    "        code = self.encode(X).cpu()\n",
    "        if fit:\n",
    "            self.isolation_forest.fit(code)\n",
    "        return self.isolation_forest.predict(code) # return +1 if inlier and -1 otherwise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN CODE FROM HERE - REQUIRES TORCHQUANTUM\n",
    "## Using TorchQuantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchquantum as tq\n",
    "import torchquantum.functional as tqf\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "class HAEBottleneck(tq.QuantumModule):\n",
    "    # initially non parametrized, circuit 10 with 3 reps\n",
    "    def __init__(self, nqubits: int = 4, n_reps: int = 3):\n",
    "\n",
    "        def create_blocks(nqubits: int):\n",
    "            return [(tq.RY(has_params=True, trainable=True), tq.RX(has_params=True, trainable=True)) for _ in range(nqubits)]\n",
    "\n",
    "        def get_encoder_list(nqubits: int):\n",
    "            return [{'input_idx': [i], 'func': 'rx', 'wires': [i]} for i in range(nqubits)]\n",
    "\n",
    "        def get_observables(nqubits: int):\n",
    "            return [[tq.PauliZ() if j == i else tq.I() for j in range(nqubits)] for i in range(nqubits)]\n",
    "        \n",
    "        super(HAEBottleneck, self).__init__()\n",
    "        \n",
    "        self.qd = tq.QuantumDevice(n_wires=nqubits) \n",
    "        self.n_qubits = nqubits\n",
    "        self.n_reps = n_reps\n",
    "        self.encoder = tq.GeneralEncoder(get_encoder_list(nqubits))\n",
    "        self.measure = tq.MeasureAll(obs=tq.PauliZ)\n",
    "        # self.observables = get_observables(nqubits)\n",
    "        self.blocks: list[ # block-wise\n",
    "            list[ # qubit-wise\n",
    "            tuple[tq.RY, tq.RX]]] = [create_blocks(nqubits) for _ in range(n_reps)]\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "\n",
    "        def apply_block(block: list[tuple[tq.RY, tq.RX]]):\n",
    "            for qubit, gates in enumerate(block):\n",
    "                (gates[0])(self.qd, wires=qubit) # Y gate\n",
    "                (gates[1])(self.qd, wires=qubit) # X gate\n",
    "\n",
    "        def cnot_block():\n",
    "            if self.n_qubits == 1:\n",
    "                return\n",
    "            tqf.cnot(self.qd, wires=[self.n_qubits - 1, 0])\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                tqf.cnot(self.qd, wires=[i, i + 1])\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        # BLOCK 1\n",
    "        apply_block(self.blocks[0])\n",
    "        cnot_block()\n",
    "        # Other BLOCKS\n",
    "        for rep in range(1, len(self.blocks)):\n",
    "            self.encoder(self.qd, X)\n",
    "            apply_block(self.blocks[rep])\n",
    "            cnot_block()\n",
    "        \"\"\"     # A MORE FLEXIBLE MEASURE\n",
    "        e0 = tq.expval(self.qd, [i for i in range(4)], self.observables[0]) # 1 x B\n",
    "        e1 = tq.expval(self.qd, [i for i in range(4)], self.observables[1])\n",
    "        e2 = tq.expval(self.qd, [i for i in range(4)], self.observables[2])\n",
    "        e3 = tq.expval(self.qd, [i for i in range(4)], self.observables[3])\n",
    "        E = torch.stack((e0[:, 0], e1[:, 1], e2[:, 2], e3[:, 3]), dim=0).T  # 4 x B\n",
    "        print((e0[:, 0], e1[:, 1], e2[:, 2], e3[:, 3]))\n",
    "        \"\"\"\n",
    "        Z = self.measure(self.qd)\n",
    "        return Z\n",
    "\n",
    "\n",
    "class HAE_TQ(nn.Module):\n",
    "    \"\"\"\n",
    "        general structure is:\n",
    "        - encoder, FC input_size -> 54 -> 4\n",
    "        - qnn\n",
    "        - decoder, FC 4 -> 54 -> input_size\n",
    "        - tanh activations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, nqubits: int = 4, n_reps: int = 3) -> None:\n",
    "        super(HAE_TQ, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, nqubits),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.vqc = HAEBottleneck(nqubits, n_reps)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(nqubits, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, input_size)\n",
    "        )\n",
    "\n",
    "        self.isolation_forest = IsolationForest()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.vqc(X)\n",
    "        # X = (torch.ones_like(X) - X) / 2\n",
    "        X = self.decoder(X)\n",
    "        return X\n",
    "\n",
    "    def encode(self, X):\n",
    "        C = self.encoder(X)\n",
    "        return self.vqc(C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X, fit: bool = False):\n",
    "        code = self.encode(X).cpu()\n",
    "        if fit:\n",
    "            self.isolation_forest.fit(code)\n",
    "        return self.isolation_forest.predict(code)  # return +1 if inlier and -1 otherwiseimport torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class HAE_C(nn.Module):\n",
    "    \"\"\"\n",
    "        general structure is:\n",
    "        - encoder, FC input_size -> 54 -> 4\n",
    "        - qnn\n",
    "        - decoder, FC 4 -> 54 -> input_size\n",
    "        - tanh activations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, nqubits: int = 4) -> None:\n",
    "        super(HAE_C, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, nqubits),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(nqubits, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, nqubits)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(nqubits, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, input_size)\n",
    "        )\n",
    "\n",
    "        self.isolation_forest = IsolationForest()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.bottleneck(X)\n",
    "        # X = (torch.ones_like(X) - X) / 2\n",
    "        X = self.decoder(X)\n",
    "        return X\n",
    "\n",
    "    def encode(self, X):\n",
    "        C = self.encoder(X)\n",
    "        return self.bottlenack(C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X, fit: bool = False):\n",
    "        code = self.encode(X).cpu()\n",
    "        if fit:\n",
    "            self.isolation_forest.fit(code)\n",
    "        return self.isolation_forest.predict(code)  # return +1 if inlier and -1 otherwiseimport torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Training & Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor()\n",
    "print(a)\n",
    "a = torch.cat((a, torch.Tensor(2, 1, 1)), dim=0)\n",
    "torch.cat((a, torch.Tensor(2, 1, 1)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    mse = nn.MSELoss()\n",
    "    model.eval()\n",
    "    avg_error = 0\n",
    "    num_matches = 0\n",
    "    data = torch.Tensor().to(device)\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
    "            X = X.to(device)\n",
    "            # reconstruction error\n",
    "            reconstruction = model(X)\n",
    "            avg_error += mse(reconstruction, X).sum().item()\n",
    "            preds = torch.Tensor(model.predict(X, False))\n",
    "            num_matches += len(preds[preds == y])\n",
    "\n",
    "    return avg_error /  len(data_loader.dataset), num_matches / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def training(model, train_dl, val_dl, epochs: int = 100, log: bool = True) -> tuple[list, list, list, list]:\n",
    "    mse = nn.MSELoss()\n",
    "    optim = Adam(model.parameters(), lr=0.00001)\n",
    "    model.train()\n",
    "    if_data = torch.Tensor().to(device)\n",
    "    if_labels = []\n",
    "    model_loaded = False\n",
    "    train_reconstruction = []\n",
    "    train_classification = []\n",
    "    val_reconstruction = []\n",
    "    val_classification = []\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        avg_loss = 0\n",
    "        for X, y in tqdm(train_dl, \"Epoch #{}\".format(i), leave=False):\n",
    "            X = X.to(device)\n",
    "            if not model_loaded:\n",
    "                if_data = torch.cat((if_data, X), dim=0)\n",
    "                if_labels += y.tolist()\n",
    "            reconstruction = model(X)\n",
    "            loss = mse(reconstruction, X)\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        avg_loss/= len(train_dl.dataset)\n",
    "        model_loaded = True\n",
    "        if i % 5 == 0:\n",
    "        # first fit the isolation forest\n",
    "            train_predictions = model.predict(if_data, True)\n",
    "            num_matches = 0\n",
    "            for j in range(len(train_predictions)):\n",
    "                if train_predictions[j] == if_labels[j]:\n",
    "                    num_matches += 1\n",
    "            train_accuracy = num_matches / len(train_dl.dataset)\n",
    "\n",
    "            train_reconstruction.append(avg_loss)\n",
    "            train_classification.append(train_accuracy)\n",
    "\n",
    "            rec_error, val_accuracy = evaluate(model, val_dl)\n",
    "            val_reconstruction.append(rec_error)\n",
    "            val_classification.append(val_accuracy)\n",
    "            if log:\n",
    "                print(\n",
    "                    \"\\nEpoch:{}\\nTrain average reconstruction error: {}\\nValidation average reconstruction error: {}\\nTrain anomaly detection accuracy: {}\\nValidation anomaly detection accuracy: {}\".format(\n",
    "                        i, avg_loss, rec_error, train_accuracy, val_accuracy))\n",
    "    model.train()\n",
    "    return train_reconstruction, train_classification, val_reconstruction, val_classification\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Define a random dataset and the arrhytmia dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size, length, mean = 0, std_dev = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.values = torch.normal(mean, std_dev, size=(length, size))\n",
    "        self.labels = (torch.normal(0, 1, size=(length,)) > 0) * 1\n",
    "        self.labels.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = self.values[idx]\n",
    "        y = self.labels[idx]\n",
    "        return X, y\n",
    "    \n",
    "random_data = RandomDataset(160, 1000, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class ArrythmiaDSBalanced(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"./datasets/arrhythmia.data\", get_anomalies: bool = False) -> None:\n",
    "\n",
    "        def _is_nominal(df, idx) -> bool:\n",
    "            l = df.iloc[:, idx]\n",
    "            return len(l) == len(l[l == 0]) + len(l[l == 1])\n",
    "    \n",
    "        def _fix_missing(df: pd.DataFrame):\n",
    "            for i in range(len(df.columns)):\n",
    "                mean_value = df.iloc[:, i].mean(skipna=True)\n",
    "                if _is_nominal(df, i):\n",
    "                    # the mean is the bernoulli probability\n",
    "                    df.iloc[:, i].map(lambda x: x if x is not pd.NA else 1 * (np.random.random(mean_value) > 0.5))\n",
    "                else:\n",
    "                    pass\n",
    "                    df.iloc[:, i].fillna(value=mean_value, inplace=True)\n",
    "            return df\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.data = _fix_missing(self.data)\n",
    "\n",
    "        # get normal data or anomalies\n",
    "        if not get_anomalies:\n",
    "            self.data = self.data[self.labels == 1] \n",
    "            self.labels = self.labels[self.labels == 1] \n",
    "        else:\n",
    "            self.data = self.data[self.labels != 1] \n",
    "            self.labels = self.labels[self.labels != 1] = -1 # use label = -1 as a generic indicator of anomaly\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\n",
    "    \n",
    "\n",
    "def get_splits(dataset, dataset_anomalies, test_split: float = 0.3, validation_split: float = 0.3, batch_size: int = 64) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    I = np.random.permutation(len(dataset))\n",
    "    Ian = np.random.permutation(len(dataset_anomalies))\n",
    "    test_size = int(len(dataset) * test_split)\n",
    "    a_test_size = int(len(dataset_anomalies) * test_split)\n",
    "\n",
    "    train_val_size = int((len(dataset) - test_size) * validation_split)\n",
    "    a_train_val_size = int((len(dataset_anomalies) - a_test_size) * validation_split)\n",
    "\n",
    "    ds_test = Subset(dataset, I[:test_size]) + Subset(dataset_anomalies, Ian[:a_test_size])\n",
    "    ds_val = Subset(dataset, I[test_size: test_size + train_val_size]) + Subset(dataset_anomalies, Ian[a_test_size: a_test_size + a_train_val_size])\n",
    "    ds_train = Subset(dataset, I[test_size + train_val_size:]) + Subset(dataset_anomalies, Ian[a_test_size + a_train_val_size:])\n",
    "    return DataLoader(ds_train, batch_size=batch_size, shuffle=True), DataLoader(ds_val, batch_size=batch_size, shuffle=True), DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class ArrythmiaDS(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"datasets/arrhythmia.data\", normalize: bool = True) -> None:\n",
    "\n",
    "        def _is_nominal(df, idx) -> bool:\n",
    "            l = df.iloc[:, idx]\n",
    "            return len(l) == len(l[l == 0]) + len(l[l == 1])\n",
    "\n",
    "        def _fix_missing(df: pd.DataFrame):\n",
    "            for i in range(len(df.columns)):\n",
    "                mean_value = df.iloc[:, i].mean(skipna=True)\n",
    "                if _is_nominal(df, i):\n",
    "                    # the mean is the bernoulli probability\n",
    "                    df.iloc[:, i].map(lambda x: x if x is not pd.NA else 1 * (np.random.random(mean_value) > 0.5))\n",
    "                else:\n",
    "                    pass\n",
    "                    df.iloc[:, i].fillna(value=mean_value, inplace=True)\n",
    "            return df\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.data = _fix_missing(self.data)\n",
    "        # print(\"------------\", self.data.max() - self.data.min(), \"\\n---------------\",self.data.std())\n",
    "        # self.data = (self.data - self.data.min()) / (self.data.max() - self.data.min()) # standard normalization\n",
    "        self.data = self.data / (self.data.abs().max() + 1e-6)\n",
    "        self.labels[self.labels != 1] = -1  # use label = -1 as a generic indicator of anomaly\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\n",
    "    \n",
    "def get_splits(dataset, test_split: float = 0.3, validation_split: float = 0.3, batch_size: int = 64) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    I = np.random.permutation(len(dataset))\n",
    "    test_size = int(len(dataset) * test_split)\n",
    "    train_val_size = int((len(dataset) - test_size) * validation_split)\n",
    "    ds_test = Subset(dataset, I[:test_size])\n",
    "    ds_val = Subset(dataset, I[test_size: test_size + train_val_size])\n",
    "    ds_train = Subset(dataset, I[test_size + train_val_size:])\n",
    "    return DataLoader(ds_train, batch_size=batch_size, shuffle=True), DataLoader(ds_val, batch_size=batch_size, shuffle=True), DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class CensusDS(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"datasets/census-income-binarized.csv\") -> None:\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32, skiprows=0)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.labels[self.labels == 1] = -1  # use label = -1 as a generic indicator of anomaly\n",
    "        self.labels[self.labels == 0] = 1  # use label = -1 as a generic indicator of anomaly\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "ds = ArrythmiaDS()\n",
    "train_dl, val_dl, test_dl = get_splits(ds, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# epochs = [(i * 5) + 1 for i in range(len(tr))]\n",
    "max_q = 8\n",
    "min_q = 4\n",
    "ntest = 10\n",
    "rows = max_q - min_q + 1\n",
    "fig, ax = plt.subplots(nrows=rows, ncols=2, figsize=(15, 30))\n",
    "n_epochs = 300\n",
    "mva = [0] * rows\n",
    "a = None\n",
    "b = None\n",
    "c = None\n",
    "d = None\n",
    "for i in range(rows):\n",
    "    for j in range(ntest):\n",
    "        train_dl, val_dl, test_dl = get_splits(ds, test_split=0.01, validation_split=0.4, batch_size=8)\n",
    "        print(f\"{i+min_q} QUBITS TEST NUMBER #{j+1}/{ntest}\")\n",
    "        hae = HAE_TQ(ds[0][0].shape[0], min_q + 1).to(device)\n",
    "        tr, ta, vr, va = training(hae, train_dl, val_dl, epochs=n_epochs, log=False)\n",
    "        a = tr if a is None else [v + tr[i] for i, v in enumerate(a)] \n",
    "        b = ta if b is None else [v + ta[i] for i, v in enumerate(b)] \n",
    "        c = vr if c is None else [v + vr[i] for i, v in enumerate(c)] \n",
    "        d = va if d is None else [v + va[i] for i, v in enumerate(d)] \n",
    "        mva[i] += max(va)\n",
    "    a = [v / ntest for i, v in enumerate(a)]\n",
    "    b = [v / ntest for i, v in enumerate(b)]\n",
    "    c = [v / ntest for i, v in enumerate(c)]\n",
    "    d = [v / ntest for i, v in enumerate(d)]\n",
    "\n",
    "    epochs = [(i * 5) + 1 for i in range(len(tr))]\n",
    "    ax[i][0].set_yscale('log')\n",
    "    ax[i][0].plot(epochs, a, c='r')\n",
    "    ax[i][0].plot(epochs, c, c='b')\n",
    "    ax[i][0].legend([\"Train reconstruction error\", \"Validation reconstruction error\"])\n",
    "    ax[i][1].set_yscale('log')\n",
    "    ax[i][1].plot(epochs, b, c='r')\n",
    "    ax[i][1].plot(epochs, d, c='b')\n",
    "    ax[i][1].legend([\"Train accuracy\", \"Validation accuracy\"])\n",
    "\n",
    "mva = [v / ntest for _, v in enumerate(mva)]\n",
    "plt.show()\n",
    "print(mva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9992dec23bba76ce5f6c8e995a4c58d3becc3cbaf1a4847177dba920361e0cdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
