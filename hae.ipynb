{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Classical Quantum AutoEncoder for anomaly detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQC\n",
    "The VQC implemented is the number 10 of the reference paper, which essentially is a stack Pauli $YX$ rotation gate and a circular series of controlled $CX$, stacked while alternated with an encoding based on the Pauli $X$ rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.circuit import QuantumCircuit, Gate, ParameterVector\n",
    "from qiskit.opflow.expectations import PauliExpectation\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "def get_encoding_block(nqubits: int, features: ParameterVector) -> Gate:\n",
    "    \"\"\" n parameters required \"\"\"\n",
    "    assert len(features) == nqubits \n",
    "    block = QuantumCircuit(nqubits, name=\"Encoding Block\")\n",
    "    for i in range(nqubits):\n",
    "        block.rx(features[i], i)\n",
    "    return block.to_gate()\n",
    "\n",
    "def get_ansatz_block(nqubits: int, parameters: ParameterVector) -> Gate:\n",
    "    \"\"\"\n",
    "    need 2n parameters\n",
    "    \"\"\"\n",
    "    assert nqubits * 2 == len(parameters)\n",
    "    block = QuantumCircuit(nqubits, name=\"Ansatz Block\")\n",
    "    for i in range(nqubits):\n",
    "        block.ry(parameters[i], i)\n",
    "        block.rx(parameters[i + nqubits], i)\n",
    "    if nqubits > 1:\n",
    "        block.cx(nqubits - 1, 0)\n",
    "        for i in range(nqubits - 1):\n",
    "            block.cx(i, i + 1)\n",
    "    return block.to_gate()\n",
    "\n",
    "def get_ansatz(nqubits: int, parameters: ParameterVector, features: ParameterVector, reps: int=3) -> QuantumCircuit:\n",
    "    assert len(parameters) == reps * 2 * nqubits\n",
    "    ansatz = QuantumCircuit(nqubits)\n",
    "    ansatz.compose(get_ansatz_block(nqubits, parameters[:2 * nqubits]), range(nqubits), inplace=True)\n",
    "    for i in range(1, reps):\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_encoding_block(nqubits, features), range(nqubits), inplace=True)\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_ansatz_block(nqubits, parameters[2 * nqubits * i :2 * nqubits * (i + 1)]), range(nqubits), inplace=True)\n",
    "    return ansatz\n",
    "\n",
    "def get_ansatz_ws(nqubits: int, parameters: ParameterVector, features: ParameterVector, reps: int=3) -> QuantumCircuit:\n",
    "    assert len(parameters) == 2 * nqubits\n",
    "    ansatz = QuantumCircuit(nqubits)\n",
    "    ansatz.compose(get_ansatz_block(nqubits, parameters), range(nqubits), inplace=True)\n",
    "    for i in range(1, reps):\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_encoding_block(nqubits, features), range(nqubits), inplace=True)\n",
    "        ansatz.barrier()\n",
    "        ansatz.compose(get_ansatz_block(nqubits, parameters), range(nqubits), inplace=True)\n",
    "    return ansatz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.utils import algorithm_globals\n",
    "algorithm_globals.random_seed = 528491\n",
    "\n",
    "size = (10, 4)\n",
    "data = algorithm_globals.random.random(size)\n",
    "nqubits = data.shape[1]\n",
    "print(data, nqubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ParameterVector(\"x\", data.shape[1])\n",
    "weights = ParameterVector(\"theta\", nqubits * 2)\n",
    "circuit_ws = get_ansatz_ws(nqubits, weights, input)\n",
    "circuit_ws.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weights_ws = algorithm_globals.random.random(2 * nqubits)\n",
    "print(random_weights_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "\n",
    "sqnn_ws = SamplerQNN(\n",
    "    circuit=circuit_ws,\n",
    "    input_params=input,\n",
    "    weight_params=weights,\n",
    "    interpret=lambda x: \"{:b}\".format(x).count('1') % 2 == 0, # parity check\n",
    "    output_shape=2\n",
    ")\n",
    "print(sqnn_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_qnn_forward = sqnn_ws.forward(data[0], random_weights_ws) # require encoding + ansatz parameters, result is a ndarray\n",
    "print(f\"Forward pass result for SamplerQNN: {sampler_qnn_forward}. \\nShape: {sampler_qnn_forward.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = ParameterVector(\"x1\", data.shape[1])\n",
    "weights1 = ParameterVector(\"theta1\", nqubits * 2 * 3)\n",
    "circuit = get_ansatz(nqubits, weights1, input1)\n",
    "circuit.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity(x):\n",
    "    print(x, str(x), \"{:b}\".format(x), \"{:b}\".format(x).count('1') % 2)\n",
    "    return \"{:b}\".format(x).count('1') % 2\n",
    "\n",
    "def custom_interpret(x):\n",
    "    return \"{:b}\".format(x).count('1') % 4\n",
    "\n",
    "\n",
    "sqnn = SamplerQNN(\n",
    "    circuit=circuit,\n",
    "    input_params=input1,\n",
    "    weight_params=weights1,\n",
    "    interpret=lambda x: custom_interpret(x), # parity check\n",
    "    output_shape=4\n",
    ")\n",
    "print(sqnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weights = algorithm_globals.random.random(len(weights1))\n",
    "sampler_qnn_forward = sqnn.forward(data[0], random_weights) # require encoding + ansatz parameters, result is a ndarray\n",
    "print(f\"Forward pass result for SamplerQNN: {sampler_qnn_forward}. \\nShape: {sampler_qnn_forward.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.opflow.expectations import PauliExpectation\n",
    "\n",
    "def get_Z_expectation_qubitwise(nqubits: int) -> list:\n",
    "    \"\"\"\n",
    "        :nqubits\n",
    "        the number of qubits to evaluate\n",
    "        :return\n",
    "        the qubitwise observables for each Z expectation value, which follows the formula P(1) = (1 - Exp) / 2\n",
    "    \"\"\"\n",
    "    obs = []\n",
    "    for i in range(nqubits):\n",
    "        string = \"I\" * i + \"Z\" + \"I\" * (nqubits - (i + 1))\n",
    "        obs.append(SparsePauliOp.from_list([(string, 1)]))\n",
    "    return obs\n",
    "\n",
    "ob = get_Z_expectation_qubitwise(4)\n",
    "print(ob)\n",
    "\n",
    "# observable1 = SparsePauliOp.from_list([(\"Z\", 1), (\"Z\", 1), (\"Z\", 1), (\"I\", 1)])\n",
    "eqnn_ws = EstimatorQNN(\n",
    "    circuit=circuit_ws,\n",
    "    input_params=input,\n",
    "    weight_params=weights,\n",
    "    observables=ob\n",
    ")\n",
    "\n",
    "print(eqnn_ws)\n",
    "circuit_ws.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eqnn_ws.forward(data[0], random_weights_ws)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic AutoEncoder Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class HAE(nn.Module):\n",
    "    \"\"\"\n",
    "        general structure is:\n",
    "        - encoder, FC input_size -> 54 -> 4\n",
    "        - qnn\n",
    "        - decoder, FC 4 -> 54 -> input_size\n",
    "        - tanh activations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qnn, input_size: int, nqubits: int = 4) -> None:\n",
    "        super(HAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, nqubits),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.vqc = TorchConnector(qnn)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(nqubits, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, input_size)\n",
    "        )\n",
    "\n",
    "        self.isolation_forest = IsolationForest()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.vqc(X)\n",
    "        X = (torch.ones_like(X) - X) / 2\n",
    "        X = self.decoder(X)\n",
    "        return X\n",
    "    \n",
    "    def encode(self, X):\n",
    "        C = self.encoder(X)\n",
    "        return self.vqc(C)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, X, fit: bool = False):\n",
    "        code = self.encode(X).cpu()\n",
    "        if fit:\n",
    "            self.isolation_forest.fit(code)\n",
    "        return self.isolation_forest.predict(code) # return +1 if inlier and -1 otherwise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TorchQuantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchquantum as tq\n",
    "import torchquantum.functional as tqf\n",
    "\n",
    "def get_encoder(nqubits):\n",
    "    pass\n",
    "\n",
    "def get_ansatz(nqubits):\n",
    "    pass\n",
    "\n",
    "def get_block(nqubits, reps):\n",
    "\n",
    "class HAEBottleneck(nn.Module):\n",
    "    def __init__(self, nqubits: int = 4, reps):\n",
    "        self.nqubits = nqubits\n",
    "        self.quantum_device = tq.QuantumDevice(self.nqubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = 4\n",
    "input = torch.rand((100, ins))\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "test = HAE(eqnn_ws, input_size=279).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Training & Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor()\n",
    "print(a)\n",
    "a = torch.cat((a, torch.Tensor(2, 1, 1)), dim=0)\n",
    "torch.cat((a, torch.Tensor(2, 1, 1)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    mse = nn.MSELoss()\n",
    "    model.eval()\n",
    "    avg_error = 0\n",
    "    num_matches = 0\n",
    "    data = torch.Tensor().to(device)\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
    "            X = X.to(device)\n",
    "            # reconstruction error\n",
    "            reconstruction = model(X)\n",
    "            avg_error += mse(reconstruction, X).sum().item() / len(X)\n",
    "            data = torch.cat((data, X), dim=0)\n",
    "            labels += y.tolist()\n",
    "\n",
    "        predictions = model.predict(data, False)\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == labels[i] == 1:\n",
    "                num_matches += 1\n",
    "\n",
    "    return avg_error, num_matches / len(data_loader.dataset)\n",
    "\n",
    "def training(model, train_dl, val_dl, epochs: int = 100):\n",
    "    mse = nn.MSELoss()\n",
    "    optim = Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    if_data = torch.Tensor().to(device)\n",
    "    if_labels = []\n",
    "    model_loaded = False\n",
    "    for i in range(1, epochs + 1):\n",
    "        for X, y in tqdm(train_dl, \"Epoch #{}\".format(i), leave=True):\n",
    "            X = X.to(device)\n",
    "            if not model_loaded:\n",
    "                if_data = torch.cat((if_data, X), dim=0) \n",
    "                if_labels += y.tolist() \n",
    "            reconstruction = model(X)\n",
    "            loss = mse(reconstruction, X)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        model_loaded = True\n",
    "        # if i % 5 == 0:\n",
    "        # first fit the isolation forest\n",
    "        train_predictions = model.predict(if_data, True)\n",
    "        num_matches = 0\n",
    "        for i in range(len(train_predictions)):\n",
    "            if train_predictions[i] == if_labels[i] == 1:\n",
    "                num_matches += 1\n",
    "        train_accuracy = num_matches / len(train_dl.dataset)\n",
    "        rec_error, val_accuracy = evaluate(model, val_dl)\n",
    "        print(\"Validation average reconstruction error: {}\\nTrain anomaly detection accuracy: {}\\nValidation anomaly detection accuracy: {}\".format(rec_error, train_accuracy, val_accuracy))\n",
    "        model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Define a random dataset and the arrhytmia dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size, length, mean = 0, std_dev = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.values = torch.normal(mean, std_dev, size=(length, size))\n",
    "        self.labels = (torch.normal(0, 1, size=(length,)) > 0) * 1\n",
    "        self.labels.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = self.values[idx]\n",
    "        y = self.labels[idx]\n",
    "        return X, y\n",
    "    \n",
    "random_data = RandomDataset(160, 1000, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\"\"\"class ArrythmiaDS(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"./datasets/arrhythmia.data\", get_anomalies: bool = False) -> None:\n",
    "\n",
    "        def _is_nominal(df, idx) -> bool:\n",
    "            l = df.iloc[:, idx]\n",
    "            return len(l) == len(l[l == 0]) + len(l[l == 1])\n",
    "    \n",
    "        def _fix_missing(df: pd.DataFrame):\n",
    "            for i in range(len(df.columns)):\n",
    "                mean_value = df.iloc[:, i].mean(skipna=True)\n",
    "                if _is_nominal(df, i):\n",
    "                    # the mean is the bernoulli probability\n",
    "                    df.iloc[:, i].map(lambda x: x if x is not pd.NA else 1 * (np.random.random(mean_value) > 0.5))\n",
    "                else:\n",
    "                    pass\n",
    "                    df.iloc[:, i].fillna(value=mean_value, inplace=True)\n",
    "            return df\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.data = _fix_missing(self.data)\n",
    "\n",
    "        # get normal data or anomalies\n",
    "        if not get_anomalies:\n",
    "            self.data = self.data[self.labels == 1] \n",
    "            self.labels = self.labels[self.labels == 1] \n",
    "        else:\n",
    "            self.data = self.data[self.labels != 1] \n",
    "            self.labels = self.labels[self.labels != 1] * 0 # use label = 0 as a generic indicator of anomaly\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\"\"\"\n",
    "    \n",
    "\n",
    "\"\"\"def get_splits(dataset, dataset_anomalies, test_split: float = 0.3, validation_split: float = 0.3, batch_size: int = 64) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    I = np.random.permutation(len(dataset))\n",
    "    Ian = np.random.permutation(len(dataset_anomalies))\n",
    "    test_size = int(len(dataset) * test_split)\n",
    "    a_test_size = int(len(dataset_anomalies) * test_split)\n",
    "    train_val_size = int((len(dataset) - test_size) * validation_split)\n",
    "    a_train_val_size = int((len(dataset_anomalies) - a_test_size) * validation_split)\n",
    "    ds_test = Subset(dataset, I[:test_size]) + Subset(dataset_anomalies, Ian[:a_test_size])\n",
    "    ds_val = Subset(dataset, I[test_size: test_size + train_val_size]) + Subset(dataset_anomalies, Ian[a_test_size: a_test_size + a_train_val_size])\n",
    "    ds_train = Subset(dataset, I[test_size + train_val_size:])\n",
    "    return DataLoader(ds_train, batch_size=batch_size, shuffle=True), DataLoader(ds_val, batch_size=batch_size, shuffle=True), DataLoader(ds_test, batch_size=batch_size, shuffle=True)\"\"\"\n",
    "\n",
    "class ArrythmiaDS(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"./datasets/arrhythmia.data\") -> None:\n",
    "\n",
    "        def _is_nominal(df, idx) -> bool:\n",
    "            l = df.iloc[:, idx]\n",
    "            return len(l) == len(l[l == 0]) + len(l[l == 1])\n",
    "    \n",
    "        def _fix_missing(df: pd.DataFrame):\n",
    "            for i in range(len(df.columns)):\n",
    "                mean_value = df.iloc[:, i].mean(skipna=True)\n",
    "                if _is_nominal(df, i):\n",
    "                    # the mean is the bernoulli probability\n",
    "                    df.iloc[:, i].map(lambda x: x if x is not pd.NA else 1 * (np.random.random(mean_value) > 0.5))\n",
    "                else:\n",
    "                    pass\n",
    "                    df.iloc[:, i].fillna(value=mean_value, inplace=True)\n",
    "            return df\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.data = _fix_missing(self.data)\n",
    "    \n",
    "        self.labels[self.labels != 1] = 0 # use label = 0 as a generic indicator of anomaly\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "def get_splits(dataset, test_split: float = 0.3, validation_split: float = 0.3, batch_size: int = 64) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    I = np.random.permutation(len(dataset))\n",
    "    test_size = int(len(dataset) * test_split)\n",
    "    train_val_size = int((len(dataset) - test_size) * validation_split)\n",
    "    ds_test = Subset(dataset, I[:test_size])\n",
    "    ds_val = Subset(dataset, I[test_size: test_size + train_val_size])\n",
    "    ds_train = Subset(dataset, I[test_size + train_val_size:])\n",
    "    return DataLoader(ds_train, batch_size=batch_size, shuffle=True), DataLoader(ds_val, batch_size=batch_size, shuffle=True), DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ds = ArrythmiaDS()\n",
    "train_dl, val_dl, test_dl = get_splits(ds, batch_size=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick test\n",
    "We will only use two qubits as latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dl))\n",
    "print(train_features.shape, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import Estimator\n",
    "reps = 3\n",
    "nqubits = 2\n",
    "weights = ParameterVector(\"w\", 2 * nqubits * reps) # 2n * reps\n",
    "inputs = ParameterVector(\"x\", nqubits) # n\n",
    "qnn_circuit = get_ansatz(nqubits, parameters=weights, features=inputs, reps=reps)\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qnn_circuit,\n",
    "    input_params=inputs,\n",
    "    weight_params=weights,\n",
    "    input_gradients=True,\n",
    "    observables=get_Z_expectation_qubitwise(nqubits),\n",
    "    # estimator=Estimator(options={'shots': 20})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hae = HAE(qnn, 279, nqubits=nqubits).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(hae, train_dl, val_dl, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchquantum as tq\n",
    "import torchquantum.functional as tqf\n",
    "\n",
    "def get_encoder(nqubits):\n",
    "    pass\n",
    "\n",
    "def get_ansatz(nqubits):\n",
    "    pass\n",
    "\n",
    "def get_block(nqubits, reps):\n",
    "\n",
    "class HAEBottleneck(nn.Module):\n",
    "    def __init__(self, nqubits: int = 4, reps):\n",
    "        self.nqubits = nqubits\n",
    "        self.quantum_device = tq.QuantumDevice(self.nqubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchquantum as tq\n",
    "import torchquantum.functional as tqf\n",
    "import torch.nn as nn\n",
    "\n",
    "class HAEBottleneck(nn.Module):\n",
    "    # initially non parametrized, circuit 10 with 3 reps\n",
    "    def __init__(self):\n",
    "        super(HAEBottleneck, self).__init__()\n",
    "        self.qd = tq.QuantumDevice(n_wires=4) # 4 is the number of qubits\n",
    "        self.encoder = [tqf.rx] * 4\n",
    "        self.measure = tq.MeasureAll(tq.PauliZ)\n",
    "\n",
    "        # BLOCK 1\n",
    "        self.ansatz_1_y_1 = tq.ry(has_param=True, trainable=True)\n",
    "        self.ansatz_1_x_1 = tq.rx(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_1_y_2 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_1_x_2 = tq.rx(has_params=True, trainable=True)\n",
    "        \n",
    "        self.ansatz_1_y_3 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_1_x_3 = tq.rx(has_params=True, trainable=True)\n",
    "        \n",
    "        self.ansatz_1_y_4 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_1_x_4 = tq.rx(has_params=True, trainable=True)\n",
    "\n",
    "        # BLOCK 2\n",
    "        self.ansatz_2_y_1 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_1 = tq.rx(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_2_y_2 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_2 = tq.rx(has_params=True, trainable=True)\n",
    "        \n",
    "        self.ansatz_2_y_3 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_3 = tq.rx(has_params=True, trainable=True)\n",
    "        \n",
    "        self.ansatz_2_y_4 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_4 = tq.rx(has_params=True, trainable=True)\n",
    "\n",
    "        # BLOCK 3\n",
    "        self.ansatz_3_y_1 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_1 = tq.rx(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_3_y_2 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_2 = tq.rx(has_params=True, trainable=True)\n",
    "        \n",
    "        self.ansatz_3_y_3 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_3 = tq.rx(has_params=True, trainable=True)\n",
    "        \n",
    "        self.ansatz_3_y_4 = tq.ry(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_4 = tq.rx(has_params=True, trainable=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        # BLOCK 1\n",
    "        self.ansatz_1_y_1(self.qd, wires=0)\n",
    "        self.ansatz_1_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_1_y_2(self.qd, wires=1)\n",
    "        self.ansatz_1_x_2(self.qd, wires=1)\n",
    "        \n",
    "        self.ansatz_1_y_3(self.qd, wires=2)\n",
    "        self.ansatz_1_x_3(self.qd, wires=2)\n",
    "        \n",
    "        self.ansatz_1_y_4(self.qd, wires=3)\n",
    "        self.ansatz_1_x_4(self.qd, wires=3)\n",
    "\n",
    "        self.qd.cnot(wires=[3, 0])\n",
    "        self.qd.cnot(wires=[0, 1])\n",
    "        self.qd.cnot(wires=[1, 2])\n",
    "        self.qd.cnot(wires=[2, 3])\n",
    "\n",
    "        # ENCODER\n",
    "        for i, gate in enumerate(self.encoder):\n",
    "            gate(self.qd, wires=i, params=X[:, i])\n",
    "\n",
    "        # BLOCK 2\n",
    "        self.ansatz_2_y_1(self.qd, wires=0)\n",
    "        self.ansatz_2_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_2_y_2(self.qd, wires=1)\n",
    "        self.ansatz_2_x_2(self.qd, wires=1)\n",
    "        \n",
    "        self.ansatz_2_y_3(self.qd, wires=2)\n",
    "        self.ansatz_2_x_3(self.qd, wires=2)\n",
    "        \n",
    "        self.ansatz_2_y_4(self.qd, wires=3)\n",
    "        self.ansatz_2_x_4(self.qd, wires=3)\n",
    "\n",
    "        self.qd.cnot(wires=[3, 0])\n",
    "        self.qd.cnot(wires=[0, 1])\n",
    "        self.qd.cnot(wires=[1, 2])\n",
    "        self.qd.cnot(wires=[2, 3])\n",
    "\n",
    "        # ENCODER\n",
    "        for i, gate in enumerate(self.encoder):\n",
    "            gate(self.qd, wires=i, params=X[:, i])\n",
    "\n",
    "        # BLOCK 1\n",
    "        self.ansatz_3_y_1(self.qd, wires=0)\n",
    "        self.ansatz_3_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_3_y_2(self.qd, wires=1)\n",
    "        self.ansatz_3_x_2(self.qd, wires=1)\n",
    "        \n",
    "        self.ansatz_3_y_3(self.qd, wires=2)\n",
    "        self.ansatz_3_x_3(self.qd, wires=2)\n",
    "        \n",
    "        self.ansatz_3_y_4(self.qd, wires=3)\n",
    "        self.ansatz_3_x_4(self.qd, wires=3)\n",
    "\n",
    "        self.qd.cnot(wires=[3, 0])\n",
    "        self.qd.cnot(wires=[0, 1])\n",
    "        self.qd.cnot(wires=[1, 2])\n",
    "        self.qd.cnot(wires=[2, 3])\n",
    "\n",
    "        X = self.measure(self.qd).reshape(batch_size, 4)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "input = torch.rand((10, 4))\n",
    "m = HAEBottleneck()\n",
    "output = m(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hae.state_dict(), \"weights/hae_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.load(\"weights/hae_3reps_160.pt\")\n",
    "print(type(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hae.load_state_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(hae, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(test_dl):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchquantum as tq\n",
    "import torchquantum.functional as tqf\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "encoder_list = [\n",
    "    {'input_idx': [0], 'func': 'rx', 'wires': [0]},\n",
    "    {'input_idx': [1], 'func': 'rx', 'wires': [1]},\n",
    "    {'input_idx': [2], 'func': 'rx', 'wires': [2]},\n",
    "    {'input_idx': [3], 'func': 'rx', 'wires': [3]}\n",
    "]\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    mse = nn.MSELoss()\n",
    "    model.eval()\n",
    "    avg_error = 0\n",
    "    num_matches = 0\n",
    "    data = torch.Tensor().to(device)\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
    "            X = X.to(device)\n",
    "            # reconstruction error\n",
    "            reconstruction = model(X)\n",
    "            avg_error += mse(reconstruction, X).sum().item() / len(X)\n",
    "            data = torch.cat((data, X), dim=0)\n",
    "            labels += y.tolist()\n",
    "\n",
    "        predictions = model.predict(data, False)\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == labels[i]:\n",
    "                num_matches += 1\n",
    "\n",
    "    return avg_error, num_matches / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def training(model, train_dl, val_dl, epochs: int = 100):\n",
    "    mse = nn.MSELoss()\n",
    "    optim = Adam(model.parameters(), lr=0.00001)\n",
    "    model.train()\n",
    "    if_data = torch.Tensor().to(device)\n",
    "    if_labels = []\n",
    "    model_loaded = False\n",
    "    for i in range(1, epochs + 1):\n",
    "        for X, y in tqdm(train_dl, \"Epoch #{}\".format(i), leave=True):\n",
    "            X = X.to(device)\n",
    "            if not model_loaded:\n",
    "                if_data = torch.cat((if_data, X), dim=0)\n",
    "                if_labels += y.tolist()\n",
    "            reconstruction = model(X)\n",
    "            loss = mse(reconstruction, X)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        model_loaded = True\n",
    "        # if i % 5 == 0:\n",
    "        # first fit the isolation forest\n",
    "        train_predictions = model.predict(if_data, True)\n",
    "        num_matches = 0\n",
    "        for i in range(len(train_predictions)):\n",
    "            if train_predictions[i] == if_labels[i] == 1:\n",
    "                num_matches += 1\n",
    "        train_accuracy = num_matches / len(train_dl.dataset)\n",
    "        rec_error, val_accuracy = evaluate(model, val_dl)\n",
    "        print(\n",
    "            \"\\nValidation average reconstruction error: {}\\nTrain anomaly detection accuracy: {}\\nValidation anomaly detection accuracy: {}\".format(\n",
    "                rec_error, train_accuracy, val_accuracy))\n",
    "        model.train()\n",
    "\n",
    "\n",
    "class HAEBottleneck(tq.QuantumModule):\n",
    "    # initially non parametrized, circuit 10 with 3 reps\n",
    "    def __init__(self):\n",
    "        super(HAEBottleneck, self).__init__()\n",
    "        self.qd = tq.QuantumDevice(n_wires=4)  # 4 is the number of qubits\n",
    "        self.encoder = tq.GeneralEncoder(encoder_list)\n",
    "        self.measure = tq.MeasureAll(obs=tq.PauliZ)\n",
    "        self.observables = [\n",
    "            [tq.PauliZ(), tq.I(), tq.I(), tq.I()],\n",
    "            [tq.I(), tq.PauliZ(), tq.I(), tq.I()],\n",
    "            [tq.I(), tq.I(), tq.PauliZ(), tq.I()],\n",
    "            [tq.I(), tq.I(), tq.I(), tq.PauliZ()]\n",
    "        ]\n",
    "\n",
    "        # BLOCK 1\n",
    "        self.ansatz_1_y_1 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_1_x_1 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_1_y_2 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_1_x_2 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_1_y_3 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_1_x_3 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_1_y_4 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_1_x_4 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        # BLOCK 2\n",
    "        self.ansatz_2_y_1 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_1 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_2_y_2 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_2 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_2_y_3 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_3 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_2_y_4 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_2_x_4 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        # BLOCK 3\n",
    "        self.ansatz_3_y_1 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_1 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_3_y_2 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_2 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_3_y_3 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_3 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_3_y_4 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_3_x_4 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        # BLOCK 4\n",
    "        self.ansatz_4_y_1 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_4_x_1 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_4_y_2 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_4_x_2 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_4_y_3 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_4_x_3 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_4_y_4 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_4_x_4 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        # BLOCK 4\n",
    "        self.ansatz_5_y_1 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_5_x_1 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_5_y_2 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_5_x_2 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_5_y_3 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_5_x_3 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "        self.ansatz_5_y_4 = tq.RY(has_params=True, trainable=True)\n",
    "        self.ansatz_5_x_4 = tq.RX(has_params=True, trainable=True)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        def cnot_block():\n",
    "            tqf.cnot(self.qd, wires=[3, 0])\n",
    "            tqf.cnot(self.qd, wires=[0, 1])\n",
    "            tqf.cnot(self.qd, wires=[1, 2])\n",
    "            tqf.cnot(self.qd, wires=[2, 3])\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        # BLOCK 1\n",
    "        self.ansatz_1_y_1(self.qd, wires=0)\n",
    "        self.ansatz_1_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_1_y_2(self.qd, wires=1)\n",
    "        self.ansatz_1_x_2(self.qd, wires=1)\n",
    "\n",
    "        self.ansatz_1_y_3(self.qd, wires=2)\n",
    "        self.ansatz_1_x_3(self.qd, wires=2)\n",
    "\n",
    "        self.ansatz_1_y_4(self.qd, wires=3)\n",
    "        self.ansatz_1_x_4(self.qd, wires=3)\n",
    "\n",
    "        cnot_block()\n",
    "\n",
    "        # ENCODER\n",
    "        self.encoder(self.qd, X)\n",
    "\n",
    "        # BLOCK 2\n",
    "        self.ansatz_2_y_1(self.qd, wires=0)\n",
    "        self.ansatz_2_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_2_y_2(self.qd, wires=1)\n",
    "        self.ansatz_2_x_2(self.qd, wires=1)\n",
    "\n",
    "        self.ansatz_2_y_3(self.qd, wires=2)\n",
    "        self.ansatz_2_x_3(self.qd, wires=2)\n",
    "\n",
    "        self.ansatz_2_y_4(self.qd, wires=3)\n",
    "        self.ansatz_2_x_4(self.qd, wires=3)\n",
    "\n",
    "        cnot_block()\n",
    "\n",
    "        # ENCODER\n",
    "        self.encoder(self.qd, X)\n",
    "\n",
    "        # BLOCK 1\n",
    "        self.ansatz_3_y_1(self.qd, wires=0)\n",
    "        self.ansatz_3_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_3_y_2(self.qd, wires=1)\n",
    "        self.ansatz_3_x_2(self.qd, wires=1)\n",
    "\n",
    "        self.ansatz_3_y_3(self.qd, wires=2)\n",
    "        self.ansatz_3_x_3(self.qd, wires=2)\n",
    "\n",
    "        self.ansatz_3_y_4(self.qd, wires=3)\n",
    "        self.ansatz_3_x_4(self.qd, wires=3)\n",
    "        # ENCODER\n",
    "        self.encoder(self.qd, X)\n",
    "\n",
    "        # BLOCK 1\n",
    "        self.ansatz_4_y_1(self.qd, wires=0)\n",
    "        self.ansatz_4_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_4_y_2(self.qd, wires=1)\n",
    "        self.ansatz_4_x_2(self.qd, wires=1)\n",
    "\n",
    "        self.ansatz_4_y_3(self.qd, wires=2)\n",
    "        self.ansatz_4_x_3(self.qd, wires=2)\n",
    "\n",
    "        self.ansatz_4_y_4(self.qd, wires=3)\n",
    "        self.ansatz_4_x_4(self.qd, wires=3)\n",
    "        # ENCODER\n",
    "        self.encoder(self.qd, X)\n",
    "\n",
    "        # BLOCK 1\n",
    "        self.ansatz_5_y_1(self.qd, wires=0)\n",
    "        self.ansatz_5_x_1(self.qd, wires=0)\n",
    "\n",
    "        self.ansatz_5_y_2(self.qd, wires=1)\n",
    "        self.ansatz_5_x_2(self.qd, wires=1)\n",
    "\n",
    "        self.ansatz_5_y_3(self.qd, wires=2)\n",
    "        self.ansatz_5_x_3(self.qd, wires=2)\n",
    "\n",
    "        self.ansatz_5_y_4(self.qd, wires=3)\n",
    "        self.ansatz_5_x_4(self.qd, wires=3)\n",
    "\n",
    "        \"\"\"\n",
    "        e0 = tq.expval(self.qd, [i for i in range(4)], self.observables[0]) # 1 x B\n",
    "        e1 = tq.expval(self.qd, [i for i in range(4)], self.observables[1])\n",
    "        e2 = tq.expval(self.qd, [i for i in range(4)], self.observables[2])\n",
    "        e3 = tq.expval(self.qd, [i for i in range(4)], self.observables[3])\n",
    "        E = torch.stack((e0[:, 0], e1[:, 1], e2[:, 2], e3[:, 3]), dim=0).T  # 4 x B\n",
    "        print((e0[:, 0], e1[:, 1], e2[:, 2], e3[:, 3]))\"\"\"\n",
    "        Z = self.measure(self.qd)\n",
    "        \"\"\"print(Z, E-Z)\n",
    "\n",
    "        X = torch.stack((e0[:, 0], e1[:, 1], e2[:, 2], e3[:, 3]), dim=0) # 4 x B\n",
    "        print(X.shape, X)\"\"\"\n",
    "        return Z\n",
    "\n",
    "\n",
    "class HAE(nn.Module):\n",
    "    \"\"\"\n",
    "        general structure is:\n",
    "        - encoder, FC input_size -> 54 -> 4\n",
    "        - qnn\n",
    "        - decoder, FC 4 -> 54 -> input_size\n",
    "        - tanh activations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, nqubits: int = 4) -> None:\n",
    "        super(HAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, nqubits),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.vqc = HAEBottleneck()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(nqubits, 54),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(54, input_size)\n",
    "        )\n",
    "\n",
    "        self.isolation_forest = IsolationForest()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        X = self.vqc(X)\n",
    "        # X = (torch.ones_like(X) - X) / 2\n",
    "        X = self.decoder(X)\n",
    "        return X\n",
    "\n",
    "    def encode(self, X):\n",
    "        C = self.encoder(X)\n",
    "        return self.vqc(C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X, fit: bool = False):\n",
    "        code = self.encode(X).cpu()\n",
    "        if fit:\n",
    "            self.isolation_forest.fit(code)\n",
    "        return self.isolation_forest.predict(code)  # return +1 if inlier and -1 otherwise\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"class ArrythmiaDS(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"./datasets/arrhythmia.data\", get_anomalies: bool = False) -> None:\n",
    "\n",
    "        def _is_nominal(df, idx) -> bool:\n",
    "            l = df.iloc[:, idx]\n",
    "            return len(l) == len(l[l == 0]) + len(l[l == 1])\n",
    "\n",
    "        def _fix_missing(df: pd.DataFrame):\n",
    "            for i in range(len(df.columns)):\n",
    "                mean_value = df.iloc[:, i].mean(skipna=True)\n",
    "                if _is_nominal(df, i):\n",
    "                    # the mean is the bernoulli probability\n",
    "                    df.iloc[:, i].map(lambda x: x if x is not pd.NA else 1 * (np.random.random(mean_value) > 0.5))\n",
    "                else:\n",
    "                    pass\n",
    "                    df.iloc[:, i].fillna(value=mean_value, inplace=True)\n",
    "            return df\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.data = _fix_missing(self.data)\n",
    "\n",
    "        # get normal data or anomalies\n",
    "        if not get_anomalies:\n",
    "            self.data = self.data[self.labels == 1] \n",
    "            self.labels = self.labels[self.labels == 1] \n",
    "        else:\n",
    "            self.data = self.data[self.labels != 1] \n",
    "            self.labels = self.labels[self.labels != 1] * 0 # use label = 0 as a generic indicator of anomaly\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\"\"\"\n",
    "\n",
    "\"\"\"def get_splits(dataset, dataset_anomalies, test_split: float = 0.3, validation_split: float = 0.3, batch_size: int = 64) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    I = np.random.permutation(len(dataset))\n",
    "    Ian = np.random.permutation(len(dataset_anomalies))\n",
    "    test_size = int(len(dataset) * test_split)\n",
    "    a_test_size = int(len(dataset_anomalies) * test_split)\n",
    "    train_val_size = int((len(dataset) - test_size) * validation_split)\n",
    "    a_train_val_size = int((len(dataset_anomalies) - a_test_size) * validation_split)\n",
    "    ds_test = Subset(dataset, I[:test_size]) + Subset(dataset_anomalies, Ian[:a_test_size])\n",
    "    ds_val = Subset(dataset, I[test_size: test_size + train_val_size]) + Subset(dataset_anomalies, Ian[a_test_size: a_test_size + a_train_val_size])\n",
    "    ds_train = Subset(dataset, I[test_size + train_val_size:])\n",
    "    return DataLoader(ds_train, batch_size=batch_size, shuffle=True), DataLoader(ds_val, batch_size=batch_size, shuffle=True), DataLoader(ds_test, batch_size=batch_size, shuffle=True)\"\"\"\n",
    "\n",
    "\n",
    "class ArrythmiaDS(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"datasets/arrhythmia.data\", normalize: bool = True) -> None:\n",
    "\n",
    "        def _is_nominal(df, idx) -> bool:\n",
    "            l = df.iloc[:, idx]\n",
    "            return len(l) == len(l[l == 0]) + len(l[l == 1])\n",
    "\n",
    "        def _fix_missing(df: pd.DataFrame):\n",
    "            for i in range(len(df.columns)):\n",
    "                mean_value = df.iloc[:, i].mean(skipna=True)\n",
    "                if _is_nominal(df, i):\n",
    "                    # the mean is the bernoulli probability\n",
    "                    df.iloc[:, i].map(lambda x: x if x is not pd.NA else 1 * (np.random.random(mean_value) > 0.5))\n",
    "                else:\n",
    "                    pass\n",
    "                    df.iloc[:, i].fillna(value=mean_value, inplace=True)\n",
    "            return df\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.data = _fix_missing(self.data)\n",
    "        # print(\"------------\", self.data.max() - self.data.min(), \"\\n---------------\",self.data.std())\n",
    "        # self.data = (self.data - self.data.min()) / (self.data.max() - self.data.min()) # standard normalization\n",
    "        self.data = self.data / (self.data.abs().max() + 1e-6)\n",
    "        self.labels[self.labels != 1] = -1  # use label = -1 as a generic indicator of anomaly\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "\n",
    "class CensusDS(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = \"datasets/census-income-binarized.csv\") -> None:\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(path, sep=',', na_values='?', dtype=np.float32, skiprows=0)\n",
    "        self.labels = self.data.iloc[:, -1]\n",
    "        self.data = self.data.iloc[:, :-1]\n",
    "        self.labels[self.labels == 1] = -1  # use label = -1 as a generic indicator of anomaly\n",
    "        self.labels[self.labels == 0] = 1  # use label = -1 as a generic indicator of anomaly\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx, :]), torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "\n",
    "def get_splits(dataset, test_split: float = 0.1, validation_split: float = 0.3, batch_size: int = 4) -> tuple[\n",
    "    DataLoader, DataLoader, DataLoader]:\n",
    "    I = np.random.permutation(len(dataset))\n",
    "    test_size = int(len(dataset) * test_split)\n",
    "    train_val_size = int((len(dataset) - test_size) * validation_split)\n",
    "    ds_test = Subset(dataset, I[:test_size])\n",
    "    ds_val = Subset(dataset, I[test_size: test_size + train_val_size])\n",
    "    ds_train = Subset(dataset, I[test_size + train_val_size:])\n",
    "    return DataLoader(ds_train, batch_size=batch_size, shuffle=True), DataLoader(ds_val, batch_size=batch_size,\n",
    "                                                                                 shuffle=True), DataLoader(ds_test,\n",
    "                                                                                                           batch_size=batch_size,\n",
    "                                                                                                           shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ds = ArrythmiaDS()\n",
    "train_dl, val_dl, test_dl = get_splits(ds, batch_size=6)\n",
    "\n",
    "input = torch.rand((100, 40))\n",
    "m = HAE(input_size=279).to('cuda')\n",
    "training(m, train_dl, val_dl, epochs=200)\n",
    "torch.save(m.state_dict(), \"param.pt\")\n",
    "\"\"\"\n",
    "ds = CensusDS()\n",
    "train_dl, val_dl, test_dl = get_splits(ds, batch_size=80)\n",
    "m = HAE(input_size=ds[0][0].shape[0]).to('cuda')\n",
    "training(m, train_dl, val_dl, epochs=100)\n",
    "torch.save(m.state_dict(), \"param_census.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9992dec23bba76ce5f6c8e995a4c58d3becc3cbaf1a4847177dba920361e0cdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
